---
title: "Predicting Diabetes via health factors and logistic regression"
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
format: pdf
editor: visual
---

## Preston O'Connor, Khoa Dao, Anthony Yasan, Matthew Jacob

### 4/22/2025

### Introduction

The model predicts the incidence of diabetes based upon a number of factors including age, gender, BMI and a variety of medical conditions and history which may have an impact on diabetes risk. The data we used is of unknown source in terms of country or year, or even if it's from a singular country or year. Supposedly it’s patient data, but some of the variables have very questionable categories that don’t seem to be very medically standard (ie, the smoking categories). The data seems to have been uploaded in 2023. 

The question that our model addresses is how can doctors predict diabetes, which obviously has implications for prevention and early intervention. Jean Marx wrote in 2002 that a key predictor is obesity, and the explosion in obesity rates coincided with type 2 diabetes rates. There are a complex number of predictors, but fundamentally the actual condition of type 2 diabetes is the loss of the body’s ability to produce enough insulin or respond to it in order to control blood sugar levels, as Robinson and Turner wrote. When considering our model therefore there should be the thought process of whether or not variables are actually contributing to that process or are simply correlated with other variables either in or outside our model and thus appear to be contributing. 

The packages we used are tidyverse, which includes many tools for data analysis, GGally, which is an extension to ggplot2, and broom is used to produce nice tibbles from statistical information.

Our results indicated a high accuracy of 96.8%, meaning for 96.8% of patients it successfully predicted whether or not they had diabetes, but a low precision of 37.4%, which suggests that a substantial number of individuals predicted to have diabetes did not. Again it is a difficult issue in medical research to identify the role some variables actually play and the extent to which they are independent of each other, which may have contributed to ou

```{r}
# Upload the code pacakges here
library(tidyverse)
library(GGally)
library(broom)
library(ggplot2)
#library(corrplot) uncomment
library(dplyr)

data <- read_csv("diabetes_prediction_dataset.csv")
head(data)
```

### Data Description

The dataset used for this analysis is the **Diabetes Prediction Dataset**, sourced from Kaggle (<https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset/data>). This dataset contains several health-related features to predict the likelihood of diabetes. Below is a detailed description of the data structure, variable introductions, data size, and initial data cleaning process.

#### Data Structure:

The dataset consists of rows and columns, where each row corresponds to an individual, and the columns represent different health-related variables. The key variables (features) in the dataset are:

-   `age`: Age of the patient.

-   `gender`: Gender of the patient (binary variable: Male/Female).

-   `bmi`: Body mass index (weight in kg / height in m²).

-   `hypertension`: Whether the individual has hypertension (binary variable: 1 for yes, 0 for no).

-   `heart_disease`: Whether the individual has a history of heart disease (binary variable: 1 for yes, 0 for no).

-   `smoking_history`: History of smoking (binary variable: 1 for smoker, 0 for non-smoker).

-   `HbA1c_level`: Hemoglobin A1c percentage, a measure of blood glucose control over time.

-   `blood_glucose_level`: Blood glucose concentration (mg/dL).

-   `diabetes`: The target variable indicating whether the individual has diabetes (1) or not (0).

#### Data Size

The dataset originally contains 100,000 observations and 9 variables.

```{r}

continuous <- data %>% 
  select(where(is.numeric))
summary(continuous)
```

Hypertension, heart_disease, diabetes are all categorical. There is no need to standardize these values. We will standardize the rest of the following available categories

#### Data Cleaning

We clean and process the dataset by handling outliers and standardizing numeric variables to improve data quality for analysis. We first remove extreme values by setting the top 1% of each numeric column to `NA`. Then, we normalize numeric features by scaling them between 0 and 1. Categorical variables are converted into factors, and rows with missing numeric values are removed to maintain consistency. We then apply the **Interquartile Range (IQR) method**, filtering out rows where key health indicators—such as **age, BMI, blood glucose, and HbA1c levels**—fall outside an acceptable range. This should result in a cleaner dataset that minimizes statistical distortions.

```{r}
#cleaned our continuous variables
remove_outliers <- function(col) {
  top_one_percent <- quantile(col, 0.99)

  col[col >= top_one_percent] <- NA  # Replace outliers with NA
  return(col)
}

# normalize all of the columns with integers
normalize_features <- function(col) {
  col_min <- min(col, na.rm = TRUE)
  col_max <- max(col, na.rm = TRUE)
  
  return((col - col_min) / (col_max - col_min))
}

cleaned_data <- data %>% 
  mutate(across(c(gender, smoking_history, hypertension, heart_disease, diabetes), as_factor), 
         across(where(is.numeric), remove_outliers)) %>% 
  filter(if_all(where(is.numeric), \(col) !is.na(col))) %>% 
  mutate(across(where(is.numeric), normalize_features))

cleaned_data
```

```{r}
# Handle outliers: Remove rows where BMI/BloodGlucose/HbA1c/Age are outliers (beyond 1.5 * IQR)
# Function to remove outliers beyond 1.5 * IQR
remove_outliers_IQR <- function(df, column) {
  Q1 <- quantile(df[[column]], 0.25, na.rm = T)
  Q3 <- quantile(df[[column]], 0.75, na.rm = T)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  df %>% filter(df[[column]] >= lower_bound & df[[column]] <= upper_bound)
}

# Remove outliers for Age, BMI, BloodGlucose, and HbA1c using the custom function
cleaned_data <- cleaned_data %>%
  remove_outliers_IQR("age") %>%
  remove_outliers_IQR("bmi") %>%
  remove_outliers_IQR("blood_glucose_level") %>%
  remove_outliers_IQR("HbA1c_level")

summary(cleaned_data)
```

#### Data Visualization

```{r}
# Histogram for Age, BMI, BloodGlucose, and HbA1c
ggplot(cleaned_data, aes(x = age)) + 
  geom_histogram(binwidth = 0.075, fill = "blue", color = "black") +
  theme_minimal() + 
  labs(title = "Age Distribution")


ggplot(cleaned_data, aes(x = bmi)) + 
  geom_histogram(binwidth = 0.075, fill = "green", color = "black") + 
  theme_minimal() + 
  labs(title = "BMI Distribution")


ggplot(cleaned_data, aes(x = blood_glucose_level)) + 
  geom_histogram(binwidth = 0.15, fill = "red", color = "black") +
  theme_minimal() + 
  labs(title = "Blood Glucose Distribution")


ggplot(cleaned_data, aes(x = HbA1c_level)) + 
  geom_histogram(binwidth = 0.15, fill = "purple", color = "black") + 
  theme_minimal() + 
  labs(title = "HbA1c Distribution")
```

```{r}
# Boxplot for Age
boxplot(cleaned_data$age, 
        main = "Boxplot of Age", 
        ylab = "Age", 
        col = "blue", 
        border = "black", 
        horizontal = FALSE)

# Boxplot for BMI
boxplot(cleaned_data$bmi, 
        main = "Boxplot of BMI", 
        ylab = "BMI", 
        col = "green", 
        border = "black", 
        horizontal = FALSE) 

# Boxplot for BloodGlucose
boxplot(cleaned_data$blood_glucose_level, 
        main = "Boxplot of Blood Glucose", 
        ylab = "Blood Glucose (mg/dL)", 
        col = "red", 
        border = "black", 
        horizontal = FALSE) 

# Boxplot for HbA1c
boxplot(cleaned_data$HbA1c_level, 
        main = "Boxplot of HbA1c", 
        ylab = "HbA1c (%)", 
        col = "purple", 
        border = "black", 
        horizontal = FALSE) 
```

```{r}
# Bar plot for Hypertension
ggplot(cleaned_data, aes(x = factor(hypertension))) +
  geom_bar(fill = "lightblue", color = "black") +
  labs(title = "Hypertension Distribution", x = "Hypertension (0 = No, 1 = Yes)", y = "Count") +
  theme_minimal()

# Bar plot for HeartDisease
ggplot(cleaned_data, aes(x = factor(heart_disease))) +
  geom_bar(fill = "lightgreen", color = "black") +
  labs(title = "Heart Disease History Distribution", x = "Heart Disease (0 = No, 1 = Yes)", y = "Count") +
  theme_minimal()

# Bar plot for SmokingHistory
ggplot(cleaned_data, aes(x = factor(smoking_history))) +
  geom_bar(fill = "lightcoral", color = "black") +
  labs(title = "Smoking History Distribution", x = "Smoking History (0 = Non-Smoker, 1 = Smoker)", y = "Count") +
  theme_minimal()

# Bar plot for Diabetes
ggplot(cleaned_data, aes(x = factor(diabetes))) +
  geom_bar(fill = "lightgrey", color = "black") +
  labs(title = "Diabetes Distribution", x = "Diabetes (0 = No Diabetes, 1 = Diabetes)", y = "Count") +
  theme_minimal()
```

### Analysis

##### Continuous variables

```{r}
continuous <- cleaned_data %>% 
  select(where(is.numeric))
summary(continuous)
```

##### Factor Variables

```{r}
cleaned_data %>%
  summarize(across(where(is.factor), n_distinct)) %>% 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "count") %>% 
  ggplot(aes(x = variable, y = count)) +
  geom_bar(stat = "identity")
    

```

From the graph above, we can see that smoking history has 6 levels. This is somewhat substantial, and some levels have relatively low number of observations. These are some values to take notice for the model.

##### Feature Engineering

```{r}
cleaned_data %>% 
  count(smoking_history)


```

Here we can see that even if we wanted to remove the no info column from our data set, there are a substantial amount of points that contribute to our models variance.

##### Summary Statistic

```{r}
cleaned_data %>% 
  count(diabetes) %>% 
  ggplot(aes(x = diabetes, y = n))+
  geom_bar(stat = "identity")
```

There is an extreme imbalance in the range of the distribution this is something to be aware of for our model implementiation and to take note of if there are any issues in our implementation.

```{r}
#| fig-width: 10
#| fig-height: 10

cleaned_data %>% 
  sample_n(size = 100) %>% 
  ggpairs(columns = 1:ncol(cleaned_data), diag = "blankDiag", upper = list(continuous = "points"))
```

The diabetes column has a well-disperse binary target , so it's a good fit for classification, and its correlations with features like BMI, level of HbA1c, and blood glucose present strong patterns for prediction.

##### Train/Test & Building Model

```{r}
set.seed(1234)

create_train_test <- function(data, size = 0.8, train = TRUE) {
  n_row = nrow(data)
  total_row = size * n_row
  train_sample <- 1: total_row
  if (train == TRUE) {
    return (data[train_sample, ])
  } else {
    return (data[-train_sample, ])
    mean(glm_pred == as.character(testing$diabetes))
  }
}

data_train <- create_train_test(cleaned_data, 0.7, train = TRUE)
data_test <- create_train_test(cleaned_data, 0.7, train = FALSE)
dim(data_train)


glm_fit <- glm(diabetes ~ ., data = data_train, family = "binomial")
predictions <- predict(glm_fit, newdata = data_test, type = "response")
glm_pred <- rep("0", nrow(data_test))
glm_pred[predictions > 0.5] = "1"

table(glm_pred)
table(data_test$diabetes)
# for the accuracy mentioned further down in reference
ac <- mean(glm_pred == data_test$diabetes)

tidy(glm_fit)
```

##### Assess Model Performance

confusion matrixes for the next two

```{r}
summary(glm_fit)
```

```{r}
table(glm_pred, data_test$diabetes)
```

The model is 96.4% accurate, which means it is right about most cases. However, its precision is only 37.4%, i.e., it often predicts diabetes when the person does not have it. The recall is 78.1%, which means that it correctly classifies most actual cases of diabetes but occasionally gets them wrong. While the model is fairly accurate, its low precision suggests that it may not be very effective in avoiding false positives.

#### Null Model

```{r}
null_model <- glm(diabetes ~ 1, data = data_train, family = "binomial")

# Check the summary of the null model
summary(null_model)
```

The null model only predicts diabetes based on the proportion in the data set, without using any features. Because our intercept is negative, it suggest the models info has less occurances of diabetes.

#### Our Model

```{r}
coefs <- coef(glm_fit)
equation <- paste0("Logit(P(Y=1)) = ", round(coefs[1], 3), " + ", 
                   paste(names(coefs[-1]), round(coefs[-1], 3), sep = " * ", collapse = " + "))
wrapped_equation <- strwrap(equation, width = 80)
cat(wrapped_equation, sep = "\n")
wrapped_equation
```

\\begin{align\*}\\log \\left( \\frac{P(Y=1)}{1 - P(Y=1)} \\right) &= -15.416 + 0.223 \\cdot \\text{genderMale} \\\\&\\quad - 9.779 \\cdot \\text{genderOther} + 3.739 \\cdot \\text{age} \\\\&\\quad + 0.759 \\cdot \\text{hypertension1} + 0.87 \\cdot \\text{heart\\\_disease1} \\\\&\\quad - 0.502 \\cdot \\text{smoking\\\_historyNo\\\_Info} + 0.112 \\cdot \\text{smoking\\\_historycurrent} \\\\&\\quad + 0.135 \\cdot \\text{smoking\\\_historyformer} + 0.153 \\cdot \\text{smoking\\\_historyever} \\\\&\\quad - 0.044 \\cdot \\text{smoking\\\_historynot\\\_current} + 4.109 \\cdot \\text{bmi} \\\\&\\quad + 11.072 \\cdot \\text{HbA1c\\\_level} + 4.824 \\cdot \\text{blood\\\_glucose\\\_level}\\end{align\*}

```{r}
summary(glm_fit)
```

The logistic regression mode shows a significant reduction in deviation from 22169, found from our null, to the 11961 in our residual deviance. This showcases the the predictors variable contribute meaningful explanations for the output. The AIC of 11,989 suggest a reasonable ballance between the models complexity and fit. The model converged in 12 fisher scoring iteration, showcasing stable parameter estimation. These metric indicated the this is a well fitting model however, we can continue to asses

#### Model Comparison with Reduction in deviance

```{r}
anova(glm_fit, test = "Chisq")
```

### Application to GLM

##### BIC

```{r}
bic_value <- BIC(glm_fit)
print(paste("BIC:", bic_value))

```

The BIC assesses model fit while analyzing complexity, where lower values indicate a good balance between accuracy and parsimony. If the BIC decreases after removing unnecessary variables, the model is likely more efficient without sacrificing predictability. If the BIC increases, significant predictors may have been removed, and the model may need to be re-checked for feature selection.

### Model evaluation and predictions

##### Model Accuracy

```{r}
# model Accuracy (Full model due to no need for feature selection)
mean(glm_pred == data_test$diabetes)
# model Accuracy of the NULL
mean(null_model == data_test$diabetes)
```

The full model obtained an accuracy of 96.85%, while the null models accuracy was only 0.098%. This indicates that the full model effectively predicts diabetes, whereas the null model performs no better than random guessing

#### likelihood Ratio Test:

```{r}
anova(null_model, glm_fit, test = "Chisq")
```

The ANOVA test is used to determine whether the addition of predictors significantly improves the model's performance. The full model significantly improves diabetes prediction over our null model, from 22,169 to 11,961 residual deviance with a chi-square of 10,208 and a p value of \<2.2e-16

#### Pseudo R-Squared

```{r}
null_logLik <- logLik(null_model)

full_logLik <- logLik(glm_fit)

# McFadden's Pseudo R-squared
pseudo_r2 <- 1 - (as.numeric(full_logLik) / as.numeric(null_logLik))
print(paste("Pseudo R-squared:", pseudo_r2))
```

The pseudo R-squared of 0.4605 indicates that the full model explains about 46.05% of the variation we see with the diabetes outcomes compared to our null model. The value shows a moderately strong fit.

### Conclusion/Summary

For this project, we created a logistic regression model designed to predict diabetes via various health factors. The data used in this experiment came from the Diabetes Prediction Dataset from Kaggle, which contains certain key explanatory variables, including gender, BMI, age, hypertensive status, history of heart disease, history of smoking, A1c levels, and blood glucose levels. We began our analysis by first cleaning our data, via removing outliers and normalizing numerical variables to improve model accuracy. We then implemented our logistic regression to predict diabetes, and we evaluated our model's accuracy. Our model achieved a high accuracy of 96.8%, much higher than the model null accuracy that we found of .0009. Evaluation via examining residual deviance, AIC, a likelihood ratio test, and a pseudo R\^2 test confirm the statistical value of our chosen model. With regards to positive aspects of our project, we were able to correctly select predictors that do contribute to the high accuracy of our model. Diabetes can definitely be explained by our predictors, as shown via our verification methods. With regards to the negative aspects of our project, we definitely had a pretty low precision of only about 37.4%, and our model does have an issue with avoiding false positives. Also, our data and subsequent model lacked many variables that could add greatly to our model, including socioeconomic variables, mental health variables, lifestyle variables, and genetic variables (predisposition to diabetes, other health risks). Looking forward, our model could be improved by incorporation of those other diabetes-relevant variables. Our dataset was somewhat limited, and including more variables could definitely help us avoid type 1 error.

### References

Kassambara, Thanos, Kassambara, & Sfd. (2018, March 11). Logistic Regression Essentials in R. STHDA. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/#:%7E:text=The%20R%20function%20glm(),want%20to%20fit%20logistic%20regression

Marx, J. (2002). Unraveling the Causes of Diabetes. Science 296, 5568. http://www.jstor.org/stable/3076573

Robinson, M., & Turner, C. (2019). Incidence and prevalence of type 2 diabetes in America: Is there culpability in the food industry? State Crime Journal, 8(2). https://doi.org/10.13169/statecrime.8.2.0175

Soga. SOGA •. (2016, August 30). https://www.geo.fu-berlin.de/en/v/soga/Basics-of-statistics/Logistic-Regression/Logistic-Regression-in-R---An-Example/index.html
